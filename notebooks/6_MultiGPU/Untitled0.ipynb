{
 "metadata": {
  "name": "",
  "signature": "sha256:c46d8266c79b6abb8745d78fa40ede410d14507697b3225c9c8db783020cb341"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Processing Units logs\n",
      "log_device_placement = True\n",
      "\n",
      "#num of multiplications to perform\n",
      "n = 10\n",
      "# Example: compute A^n + B^n on 2 GPUs\n",
      "\n",
      "# Create random large matrix\n",
      "A = np.random.rand(10000, 10000).astype('float32')\n",
      "B = np.random.rand(10000, 10000).astype('float32')\n",
      "\n",
      "# Creates a graph to store results\n",
      "c1 = []\n",
      "c2 = []\n",
      "# Define matrix power\n",
      "def matpow(M, n):\n",
      "    if n < 1: #Abstract cases where n < 1\n",
      "        return M\n",
      "    else:\n",
      "        return tf.matmul(M, matpow(M, n-1))\n",
      "with tf.device('/cpu:0'):\n",
      "    a = tf.constant(A)\n",
      "    b = tf.constant(B)\n",
      "    #compute A^n and B^n and store results in c1\n",
      "    c1.append(matpow(a, n))\n",
      "    c1.append(matpow(b, n))\n",
      "    sum = tf.add_n(c1)\n",
      "    \n",
      "t1_1 = datetime.datetime.now()\n",
      "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
      "    # Runs the op.\n",
      "    sess.run(sum)\n",
      "t2_1 = datetime.datetime.now()\n",
      "print str(t2_1-t1_1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}